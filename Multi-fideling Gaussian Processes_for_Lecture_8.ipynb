{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USNCCM15 Short Course\n",
    "# SC15-005: Machine Learning Data-Driven Discretization Theories, Modeling and Applications\n",
    "\n",
    "# Hands-on tutorial on Multi-fidelity modeling with Gaussian Processes \n",
    "# Instructor: Paris Perdikaris, pgp@seas.upenn.edu\n",
    "# Code repository: https://github.com/PredictiveIntelligenceLab/USNCCM15-Short-Course-Recent-Advances-in-Physics-Informed-Deep-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear regression with Gaussian Processes\n",
    "\n",
    "The core of the multi-fidelity framework discussed in this course relies on the formulation of surrogate models  using Gaussian process regression. \n",
    "To this end, we employ a Bayesian non-parametric regression framework to model $N$ scattered scalar observations $y$ \n",
    "as a realization of a Gaussian process (GP) $f({ x})$, ${ x}\\in\\mathbb{R}^{d}$. \n",
    "\n",
    "## Prior specification\n",
    "Starting from the single-fidelity case, we can describe this observation model as: \n",
    "\n",
    "$$\n",
    "y = f({ x}) + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ represents the noise that may be currupting the observed outputs $y$ and is assumed to be a zero-mean Gaussian random variable, i.e.,\n",
    "$\\epsilon\\sim\\mathcal{N}(0,\\sigma_{n}^{2}I)$, where the variance $\\sigma_{n}^{2}$ is a-priori unkown and will be determined by the observed data.\n",
    "The prior distribution on $f({ x})$ is completely characterized by a mean function \n",
    "$\\mu({ x})$ (typically assumed to be zero), and covariance kernel function $k({ x}, { x'};\\theta)$ function,\n",
    "where $\\theta$ is a vector of hyper-parameters. Typically, the choice of the prior reflects our belief\n",
    "on the structure, regularity, and other intrinsic properties of the unkown function $f(x)$. For example, the widely used square exponential kernel function with hyper-paramters $(\\sigma_f^2, l_1^2,\\dots,l_d^2)$\n",
    "\n",
    "$$\n",
    "k({ x}, { x'};\\theta) = \\sigma_f^2 \\exp\\left(-\\frac{1}{2}\\sum\\limits_{i=1}^{d} \\frac{(x_i - x_i')^2}{l_i^2}\\right)\n",
    "$$\n",
    "is known to model smooth functions with infinitely many continuous derivatives.\n",
    "\n",
    "## Model training\n",
    "Our primary goal here is not just drawing function realizations from the Gaussian process prior prior but to incorporate the\n",
    "knowledge contained in the observations $y$ in order to learn the fucntion $f(x)$.\n",
    "Model training is performed through minimizing the negative log-marginal likelihood of the Gaussian process model (see \\cite{rasmussen2006gaussian}). In our setup, the negative log-marginal likelihood corresponsing to a zero mean GP prior be computed in a closed analytical form\n",
    "$$\n",
    "\\mathcal{L}(\\theta) :=  \\frac{1}{2}\\log|{K}+\\sigma_{n}^{2}{I}| + \\frac{1}{2}{y}^{T} ({K}+\\sigma_{n}^{2}{I})^{-1}{y} + \\frac{N}{2}\\log (2\\pi)\n",
    "$$\n",
    "where ${K}$ is a $N\\times N$ covariance matrix constructed by evaluating the kernel function $k(\\cdot,\\cdot;\\theta)$ at the locations of the input training data in the $(n\\times d)$ design matric ${X}$. The minimization here is carried out using the quasi-Newton optimizer L-BFGS, and convergence to poor local minima can be mitigated using random restarts (\\cite{liu1989limited,rasmussen2006gaussian}).\n",
    "\n",
    "## Predictive posterior distribution\n",
    "Finally, once the model has been trained on the available data, we can compute the posterior predictive distribution at a new location ${x}^{\\ast}$, namely $p({y}^{\\ast}|{x}^{\\ast},\\mathcal{D})\\sim\\mathcal{N}(\\mu({x}^{\\ast}), \\Sigma({x}^{\\ast}))$, by conditioning on the  observed data as\n",
    "\\begin{align}\n",
    "\\mu({x}^{\\ast})  & =  k({x}^{\\ast}, {X}) ({K} + \\sigma_{n}^{2})^{-1}{y} \\label{eq:posterior_mean} \\\\\n",
    "\\Sigma({x}^{\\ast}) & =  k({x}^{\\ast}, {x}^{\\ast}) - k({x}^{\\ast}, {X}) ({K} + \\sigma_{n}^{2})^{-1} k({X},{x}^{\\ast}) \\label{eq:posterior_variance}\n",
    "\\end{align}\n",
    "Where $\\mu$ represents posterior predictive distribution and $\\Sigma$ the associated uncertainty.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-fidelity Gaussian processes\n",
    "\n",
    "We can extend the single-fidelity $\\mathcal{GP}$ regression formulation to handle cases involving data coming from different information sources of variable fidelity (see \\cite{kennedy2000predicting,perdikaris2016multifidelity,perdikaris2016model}). \n",
    "\n",
    "For simplicity, here we outline the process corresponding to two levels of fidelity, although this can be generalized to arbitrarily many levels. In a two-level multi-fidelity setting we observe data \n",
    "$$\n",
    "\\mathcal{D}= [\\{({x}_{L_i}, y_{L_i})_{i=1}^{N_{L}}\\}, \\{({x}_{H_i}, y_{H_i})_{i=1}^{N_{H}}\\}] = \\{{X},{y}\\},\n",
    "$$\n",
    "where $({x}_{L}, {y}_{L})$ and $({x}_{H}, {y}_{H})$ are input/output pairs generated by a low- and high-fidelity model, respectively, typically with number of low-fidelity data $N_{L}$ and high-fidelity data $N_H$ such as $N_L>>N_{H}$. Then, our goal is to set up a multi-variate regression framework that can return accurate high-fidelity predictions while being primarily trained on low-fidelity data. To do so, we consider the following multi-output Gaussian process regression model first put forth by \\cite{kennedy2000predicting}\n",
    "\n",
    "\\begin{align}\n",
    "& {y}_{L}  =  f_{L}({x}_{L}) + \\epsilon_{L} \\\\\n",
    "& {y}_{H}  =  f_{H}({x}_{H}) + \\epsilon_{H} \\\\\n",
    "& f_{H}({x})  =  \\rho f_{L}({x}) + \\delta({x}) \\\\\n",
    "& f_{L}({x}) \\sim \\mathcal{GP}(0, k_{L}({x};{x}';\\theta_{L}), \\qquad \\epsilon_{L}\\sim\\mathcal{N}({0}, \\sigma_{n_L}^{2}{I}) \\\\ & \\delta({x}) \\sim \\mathcal{GP}(0, k_{H}({x};{x}';\\theta_{H}),\n",
    "\\qquad \\epsilon_{H}\\sim\\mathcal{N}({0}, \\sigma_{n_H}^{2}{I})\n",
    "\\end{align}\n",
    "\n",
    "where ${x}$ and ${x}'$ represent two input pairs.\n",
    "Here $f_{L}({x})$ and $\\delta({x})$ are considered to be two independent Gaussian processes, $\\rho$ is a scaling parameter that is learned during model training along with the variances $\\sigma_{n_L}^{2}$ and $\\sigma_{n_H}^{2}$ that potentially corrupt the low- and high-fidelity data, respectively. As a consequence of the auto-regressive assumption, the joint distribution of the low- and high-fidelity data inherits the following structure\n",
    "\n",
    "$$\n",
    "{y}=\\left[ \\begin{array}{c} {y}_{L} \\\\ {y}_{H} \\end{array} \\right] \n",
    "\\sim \\mathcal{N}\\left(\\left[\\begin{array}{c} {0} \\\\ {0} \\end{array} \\right],  \n",
    "\\left[ \\begin{array}{c c} {K}_{LL} & {K}_{LH}\n",
    " \\\\  & {K}_{HH}\n",
    " \\end{array} \\right]\\right) \n",
    "$$\n",
    "\n",
    "Being \n",
    "$$\n",
    "\\begin{split}\n",
    "&{K}_{LL}=k_{L}({x}_{L},{x}_{L}';\\theta_{L}) + \\sigma_{\\epsilon_{L}}^{2}{I} \\\\\n",
    "&{K}_{LH}=\\rho k_{L}({x}_{L},{x}_{H};\\theta_{L}) \\\\\n",
    "&{K}_{HH}=\\rho^2 k_{L}({x}_{H},{x}_{H}';\\theta_{L}) + k_{H}({x}_{H},{x}_{H}';\\theta_{H})  + \\sigma_{\\epsilon_{H}}^{2}{I}\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Evidently, the covariance of ${y}$ now has a block structure, where the diagonal blocks model the data in each fidelity level and the off-diagonal blocks model the cross-correlation structure between different levels of fidelity. Model training and posterior predictions can now be performed by using the concatenated low- and high-fidelity data along with this block covariance matrix structure replacing the ${K}$ matrix the operations described in the previous section. In this case, the minimization of the log-marginal likelihood in \n",
    " will return the optimal set of model parameters and hyper-parameters, namely $\\Theta=\\{\\theta_L, \\theta_H, \\rho, \\sigma_{n_L}^{2}, \\sigma_{n_H}^{2}\\}$, which can be subsequently used to perform posterior predictions by using the posterior predictive distribution presented above, albeit with the correct $K$ matrix corresponding to the multi-fidelity case. \n",
    "\n",
    "Note: The main cost associated with the construction of Gaussian process surrogates comes from inverting the dense covariance matrices appearing in the objective function that we optimize during model training. This cost scales cubically with the total number of training points, thus introducing a severe computational bottleneck in cases where a vast number of data exists. This limitation of Gaussian process regression has been effectively mitigated by algorithms presented in a recent body of literature (see \\cite{snelson2006sparse,hensman2013gaussian,raissi2017parametric,perdikaris2016multifidelity}). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python/Autograd implementation\n",
    "\n",
    "Let us know study a basic Python/Autograd implementation for a multi-fidelity Gaussian process regression with two levels of fidelity. To run this code, please make sure that the following dependencies are correctly installed:\n",
    "\n",
    "Numpy: https://github.com/numpy/numpy\n",
    "\n",
    "SciPy: https://github.com/scipy/scipy\n",
    "\n",
    "Autograd: https://github.com/HIPS/autograd\n",
    "\n",
    "PyDOE: https://github.com/tisimst/pyDOE\n",
    "\n",
    "Matplotlib: https://github.com/matplotlib/matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import autograd.numpy as np\n",
    "from autograd import value_and_grad\n",
    "from scipy.optimize import minimize\n",
    "from pyDOE import lhs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A minimal GP multi-fidelity class (two levels of fidelity)\n",
    "class Multifidelity_GP:\n",
    "    # Initialize the class\n",
    "    def __init__(self, X_L, y_L, X_H, y_H):           \n",
    "        # Number of training data\n",
    "        self.N_L = y_L.shape[0]\n",
    "        self.N_H = y_H.shape[0]\n",
    "        \n",
    "        # Concatenate data\n",
    "        X = np.concatenate([X_L, X_H], axis = 0)\n",
    "        y = np.concatenate([y_L, y_H], axis = 0)\n",
    "        \n",
    "        # Normalize data\n",
    "        self.Xmean, self.Xstd = X.mean(0), X.std(0)\n",
    "        self.Ymean, self.Ystd = y.mean(0), y.std(0)\n",
    "        X = (X - self.Xmean) / self.Xstd\n",
    "        y = (y - self.Ymean) / self.Ystd\n",
    "        \n",
    "        # Store data in class\n",
    "        self.D = X_H.shape[1]\n",
    "        self.X_L = X[:N_L,:]\n",
    "        self.y_L = y[:N_L,:]\n",
    "        self.X_H = X[N_L:,:]\n",
    "        self.y_H = y[N_L:,:]\n",
    "        \n",
    "        # Initialize hyper-parameters\n",
    "        self.hyp = self.init_params()\n",
    "        print(\"Total number of parameters: %d\" % (self.hyp.shape[0]))\n",
    "        \n",
    "        # Set jitter value to mitigate ill-conditioning\n",
    "        self.jitter = 1e-8\n",
    "\n",
    "    # Initialize hyper-parameters        \n",
    "    def init_params(self):\n",
    "        hyp = np.log(np.ones(self.D+1))\n",
    "        self.idx_theta_L = np.arange(hyp.shape[0])\n",
    "        \n",
    "        hyp = np.concatenate([hyp, np.log(np.ones(self.D+1))])\n",
    "        self.idx_theta_H = np.arange(self.idx_theta_L[-1]+1, hyp.shape[0])\n",
    "        \n",
    "        rho = np.array([1.0])\n",
    "        logsigma_n = np.array([-4.0, -4.0])\n",
    "        hyp = np.concatenate([hyp, rho, logsigma_n])\n",
    "        return hyp\n",
    "    \n",
    "    # A simple vectorized rbf kernel\n",
    "    def kernel(self,x,xp,hyp):\n",
    "        output_scale = np.exp(hyp[0])\n",
    "        lengthscales = np.exp(hyp[1:])\n",
    "        diffs = np.expand_dims(x /lengthscales, 1) - \\\n",
    "                np.expand_dims(xp/lengthscales, 0)\n",
    "        return output_scale * np.exp(-0.5 * np.sum(diffs**2, axis=2))\n",
    "        \n",
    "    # Computes the negative log-marginal likelihood\n",
    "    def likelihood(self, hyp):\n",
    "        X_L = self.X_L\n",
    "        y_L = self.y_L\n",
    "        X_H = self.X_H\n",
    "        y_H = self.y_H\n",
    "\n",
    "        y = np.vstack((y_L,y_H))\n",
    "        \n",
    "        NL = y_L.shape[0]\n",
    "        NH = y_H.shape[0]\n",
    "        N = y.shape[0]\n",
    "        \n",
    "        # Fetch hyper-parameters\n",
    "        rho = hyp[-3]\n",
    "        logsigma_n_L = hyp[-2]\n",
    "        logsigma_n_H = hyp[-1]        \n",
    "        sigma_n_L = np.exp(logsigma_n_L)\n",
    "        sigma_n_H = np.exp(logsigma_n_H)\n",
    "        \n",
    "        theta_L = hyp[self.idx_theta_L]\n",
    "        theta_H = hyp[self.idx_theta_H]\n",
    "        \n",
    "        # Construct the kernel blocks\n",
    "        K_LL = self.kernel(X_L, X_L, theta_L) + np.eye(NL)*sigma_n_L\n",
    "        K_LH = rho*self.kernel(X_L, X_H, theta_L)\n",
    "        K_HH = rho**2 * self.kernel(X_H, X_H, theta_L) + \\\n",
    "                        self.kernel(X_H, X_H, theta_H) + np.eye(NH)*sigma_n_H\n",
    "        # Assemble the covariance matrix\n",
    "        K = np.vstack((np.hstack((K_LL,K_LH)),\n",
    "                       np.hstack((K_LH.T,K_HH))))\n",
    "        # Compute Cholesky factors\n",
    "        L = np.linalg.cholesky(K + np.eye(N)*self.jitter) \n",
    "        self.L = L\n",
    "        \n",
    "        # Evaluate the negative log-marginal likelihood\n",
    "        alpha = np.linalg.solve(np.transpose(L), np.linalg.solve(L,y))    \n",
    "        NLML = 0.5*np.matmul(np.transpose(y),alpha) + \\\n",
    "               np.sum(np.log(np.diag(L))) + 0.5*np.log(2.*np.pi)*N  \n",
    "        return NLML[0,0]\n",
    "    \n",
    "    # Minimizes the negative log-marginal likelihood using L-BFGS (autodiff gradients via Autograd)\n",
    "    def train(self):\n",
    "        result = minimize(value_and_grad(self.likelihood), self.hyp, jac=True, \n",
    "                          method='L-BFGS-B', callback=self.callback)\n",
    "        self.hyp = result.x\n",
    "        \n",
    "    # Return the high-fidelity posterior mean and variance at a set of test points\n",
    "    def predict(self,X_star):\n",
    "        # Normalize inputs\n",
    "        X_star = (X_star - self.Xmean) / self.Xstd\n",
    "        \n",
    "        X_L = self.X_L\n",
    "        y_L = self.y_L\n",
    "        X_H = self.X_H\n",
    "        y_H = self.y_H\n",
    "        L = self.L\n",
    "        \n",
    "        y = np.vstack((y_L,y_H))\n",
    "        \n",
    "        # Fetch hyper-parameters\n",
    "        rho = self.hyp[-3]\n",
    "        theta_L = self.hyp[self.idx_theta_L]\n",
    "        theta_H = self.hyp[self.idx_theta_H]\n",
    "                   \n",
    "        # Construct matrices needed for computing the predictive posterior\n",
    "        psi1 = rho*self.kernel(X_star, X_L, theta_L)\n",
    "        psi2 = rho**2 * self.kernel(X_star, X_H, theta_L) + \\\n",
    "                        self.kernel(X_star, X_H, theta_H)\n",
    "        psi = np.hstack((psi1,psi2))\n",
    "\n",
    "        # Compute the predictive posterior mean\n",
    "        alpha = np.linalg.solve(np.transpose(L), np.linalg.solve(L,y))\n",
    "        pred_u_star = np.matmul(psi,alpha)\n",
    "\n",
    "        # Compute the predictive posterior covariance\n",
    "        beta = np.linalg.solve(np.transpose(L), np.linalg.solve(L,psi.T))\n",
    "        var_u_star = rho**2 * self.kernel(X_star, X_star, theta_L) + \\\n",
    "                     self.kernel(X_star, X_star, theta_H) - np.matmul(psi,beta)\n",
    "        \n",
    "        # De-normalize outputs\n",
    "        pred_u_star = pred_u_star*self.Ystd + self.Ymean\n",
    "        var_u_star = var_u_star*self.Ystd**2\n",
    "        \n",
    "        # Fix some auto-grad compatibility issue\n",
    "        if isinstance(pred_u_star, np.ndarray) == False:\n",
    "            pred_u_star = pred_u_star._value\n",
    "        if isinstance(var_u_star, np.ndarray) == False:\n",
    "            var_u_star = var_u_star._value\n",
    "            \n",
    "        return pred_u_star, var_u_star\n",
    "        \n",
    "    #  Prints the negative log-marginal likelihood at each training step         \n",
    "    def callback(self,params):\n",
    "        print(\"Log likelihood {}\".format(self.likelihood(params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example simulation\n",
    "\n",
    "Now we are ready to set up and execute our first multi-fidelity GP simulation. Here we will consider a simple synthetic dataset in one dimension given by the following low- and high-fidelity functions in the domain $x\\in[0,1]$:\n",
    "\n",
    "$$\n",
    "f_h(x) = (6x-2)^2 \\sin(12x-4),\n",
    "$$\n",
    "$$\n",
    "f_l(x) = 0.5f_h(x) + 10(x-0.5) - 5.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-fidelity data generator\n",
    "def f_H(x):\n",
    "    return (6.0*x-2.0)**2 * np.sin(12.*x-4.0)\n",
    "\n",
    "# Low-fidelity data generator\n",
    "def f_L(x):\n",
    "    return 0.5*f_H(x) + 10.0*(x-0.5) - 5.0\n",
    "\n",
    "# Number of training data\n",
    "N_H = 4\n",
    "N_L = 11\n",
    "\n",
    "# Dimension of input variables\n",
    "D = 1\n",
    "\n",
    "# Bounds of the input domain\n",
    "lb = 0.0*np.ones(D)\n",
    "ub = 1.0*np.ones(D)\n",
    "\n",
    "# Noise corrupting the training data\n",
    "noise_L = 0.00\n",
    "noise_H = 0.00\n",
    "\n",
    "    \n",
    "# Generate low-fidelity training data    \n",
    "X_L = lb + (ub-lb)*lhs(D, N_L)\n",
    "y_L = f_L(X_L)\n",
    "y_L = y_L + np.std(y_L)*noise_L*np.random.randn(N_L,1)\n",
    "\n",
    "# Generate high-fidelity training data    \n",
    "X_H = lb + (ub-lb)*lhs(D, N_H)\n",
    "y_H = f_H(X_H)\n",
    "y_H = y_H + np.std(y_H)*noise_H*np.random.randn(N_H,1)\n",
    "\n",
    "# Test data to validate the model predictions\n",
    "nn = 200\n",
    "X_star = np.linspace(lb, ub, nn)\n",
    "y_star = f_H(X_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 7\n"
     ]
    }
   ],
   "source": [
    "# Define the multi-fidelity GP model\n",
    "model = Multifidelity_GP(X_L, y_L, X_H, y_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood 7.790322061804361\n",
      "Log likelihood 7.537996086327601\n",
      "Log likelihood 6.620982935910643\n",
      "Log likelihood 5.549888677133739\n",
      "Log likelihood 4.223746694374004\n",
      "Log likelihood 1.6158616795698801\n",
      "Log likelihood 0.08350691869531168\n",
      "Log likelihood -1.0982131582695818\n",
      "Log likelihood -2.9270478554399855\n",
      "Log likelihood -4.069175362301673\n",
      "Log likelihood -4.937840971602858\n",
      "Log likelihood -5.2196214394098615\n",
      "Log likelihood -5.268521097532156\n",
      "Log likelihood -5.490614295652504\n",
      "Log likelihood -5.60059943344511\n",
      "Log likelihood -5.684485878366152\n",
      "Log likelihood -5.781093175255597\n",
      "Log likelihood -5.852538987000292\n",
      "Log likelihood -5.964300170946194\n",
      "Log likelihood -6.167502529005988\n",
      "Log likelihood -6.849560351011155\n",
      "Log likelihood -6.997625420234639\n",
      "Log likelihood -7.72170374099789\n",
      "Log likelihood -8.424294943023563\n",
      "Log likelihood -8.543215553240417\n",
      "Log likelihood -8.60580554251752\n",
      "Log likelihood -8.607707287266775\n",
      "Log likelihood -8.608386203728143\n",
      "Log likelihood -8.608747489560079\n",
      "Log likelihood -8.608966466076557\n",
      "Log likelihood -8.609180422821852\n",
      "Log likelihood -8.609503254281625\n",
      "Log likelihood -8.61017746852673\n",
      "Log likelihood -8.611887750343739\n",
      "Log likelihood -8.615834295191737\n",
      "Log likelihood -8.624390546862925\n",
      "Log likelihood -8.6383270568798\n",
      "Log likelihood -8.63947311933387\n",
      "Log likelihood -8.645060532331884\n",
      "Log likelihood -8.645211304531218\n",
      "Log likelihood -8.645271528242509\n",
      "Log likelihood -8.645547865924877\n",
      "Log likelihood -8.645654442331248\n",
      "Log likelihood -8.645754786687815\n",
      "Log likelihood -8.645841130290483\n",
      "Log likelihood -8.64607286771916\n",
      "Log likelihood -8.646442133111846\n",
      "Log likelihood -8.646898119791969\n",
      "Log likelihood -8.64690905615682\n",
      "Log likelihood -8.647156451800674\n",
      "Log likelihood -8.647211600066717\n",
      "Log likelihood -8.647222594754686\n",
      "Log likelihood -8.647227495957777\n",
      "Log likelihood -8.64725608166435\n",
      "Log likelihood -8.647314021464462\n",
      "Log likelihood -8.647433250643592\n",
      "Log likelihood -8.64753065656125\n",
      "Log likelihood -8.64754765309844\n",
      "Log likelihood -8.647582877574605\n",
      "Log likelihood -8.647590329872392\n",
      "Log likelihood -8.647602855705719\n",
      "Log likelihood -8.647607089356045\n",
      "Log likelihood -8.647607241623314\n",
      "Log likelihood -8.647611302069214\n",
      "Log likelihood -8.64761182801252\n",
      "Log likelihood -8.647612470349499\n",
      "Log likelihood -8.647612547083819\n",
      "Log likelihood -8.647612777912085\n",
      "Log likelihood -8.647612937423071\n",
      "Log likelihood -8.647613004196254\n",
      "Log likelihood -8.647613012668705\n"
     ]
    }
   ],
   "source": [
    "# Train the model with L-BFGS\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative L2 error u: 2.488585e-02\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions using the trained model\n",
    "y_pred, y_var = model.predict(X_star)\n",
    "y_var = np.abs(np.diag(y_var))\n",
    "error = np.linalg.norm(y_pred-y_star,2)/np.linalg.norm(y_star,2)\n",
    "print(\"Relative L2 error u: %e\" % (error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$f(x)$')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEGCAYAAABPdROvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deVhV1frA8e8CkUnFMWdwSHMCUXEKccw0NU3Tm2aWqVlm3Xu75S+tW5lNlpbVvVnXLOccyiE1SzM1zSlwVhzSRDMVRxAEFA7v74+D5AAIHGAfDu/nec4DZ5919n7Z4nlZa6/9LiMiKKWUUrnlZnUASimlCjdNJEoppRyiiUQppZRDNJEopZRyiCYSpZRSDilmdQB5rXz58lKjRg2rw1BKqUJl27Zt50SkQm7e63KJpEaNGkRERFgdhlJKFSrGmGO5fa8ObSmllHKIJhKllFIO0USilFLKIZpIlFJKOUQTiVJKKYdoIlFKKeUQTSRKKaUcoolEKaWUQzSRFBB3d3eCg4PTH+PHj8+zfe/cuZMVK1bk2f6UUkXM2SMOvd3l7mx3Vt7e3uzcuTNf9r1z504iIiLo1q1bvuxfKeXC5s2Ff4xwaBfaI7FQbGwsd911FwcPHgRgwIABfP755wCMGDGCkJAQGjZsyGuvvZb+nvDwcO6++24aN25MixYtiI2N5dVXX2X+/PkEBwczf/58S34WpVQhVUOgjI9j+xARl3o0a9ZMsgL587gdNzc3ady4cfpj3rx5IiKyatUqadWqlcydO1e6dOmS3v78+fMiIpKSkiLt2rWTXbt2yZUrV6RmzZry66+/iohIbGysJCcny7Rp02TkyJG3D0Ippa535YJI5ASRi1ECREguP3d1aKuAZDa01blzZ77++mtGjhzJrl270rcvWLCAKVOmkJKSwqlTp4iMjMQYQ+XKlWnevDkApUqVKrD4lVIuZu9eKLGXC779ePXfAQ7tqsglEhGrI7hRamoq+/fvx9vbmwsXLlCtWjWOHj3KxIkTCQ8Pp0yZMgwePJikpCREBGOM1SErpQq7XbsgJITjgR1pFtWPcxcd251eI7HYpEmTqF+/PnPnzmXIkCEkJydz6dIlfH198fPzIzo6mu+//x6AevXqcfLkScLDwwGIi4sjJSWFkiVLEhcXZ+WPoZQqLFJTkREjICWFpTvqcO6iOx07OrZLTSQFJDEx8Ybpv6NHj+bQoUNMnTqV999/n7CwMNq2bcubb75J48aNadKkCQ0bNmTIkCGEhoYCULx4cebPn8+zzz5L48aN6dy5M0lJSXTo0IHIyEi92K6Uui35chpm82ZOUYlXzFt8/DGsXu3YPo04yViPMeZLoAdwRkQapW0bCzwBnE1r9pKIZHnDREhIiOjCVkopdaM5c+Dl0TaOnzD4c5xq5k9GLw2lRw/768aYbSISkpt9O9M1kunAf4GZN22fJCITCz4cpZRyDXPmwPDhkJDgDsAxanDKI4DY2LzZv9MMbYnIeuCC1XEopZSrefllSEi4cdvVq4aXX86b/TtNIsnCM8aY3caYL40xZawORimlCpvjx3O2PaecPZF8CtQGgoFTwPsZNTLGDDfGRBhjIs6ePZtRE6WUKrLKls14u79/3uzfqROJiESLiE1EUoHPgRaZtJsiIiEiElKhQoWCDVIppZxYdDQkJaTest3HB956K2+O4dSJxBhT+bqnvYG9VsWilFKF0eQRe9iTWJsBVdfh7w/GQEAATJkCAwfmzTGcJpEYY+YCm4G7jDEnjDFDgfeMMXuMMbuBDsBzlgbpgGtl5Bs1akS/fv1IuPnKVw6sW7eOHmlz9pYuXZplSfqYmBgmT56c/vzkyZP07ds318dWShUeu3ZBncXvUpMoJndYyLFjkJoKUVF5l0TAie4jySvOeh9JiRIliI+PB2DgwIE0a9aMf/3rX+mvXyt+5uZ2+9y+bt06Jk6cyPLly2/bNioqih49erB3r3bmlCpKRGBgm2PM3FQbY8D96BF7VyQTjtxH4jQ9kqIkLCyMw4cPExUVRf369Xn66adp2rQpf/zxB6tWraJ169Y0bdqUfv36pSefH374gXr16tGmTRsWLVqUvq/p06fzzDPPABAdHU3v3r1p3LgxjRs3ZtOmTYwePZojR44QHBzMqFGjiIqKolGjRgAkJSXx+OOPExgYSJMmTVi7dm36Pvv06UPXrl2pU6cO//d//1fAZ0gp5aiffoLmmz6kGDZSHnwoyyTiqKKZSIzJ/DFlyl/tpkzJum0upKSk8P333xMYGAjAwYMHefTRR9mxYwe+vr68+eabrF69mu3btxMSEsIHH3xAUlISTzzxBMuWLWPDhg2cPn06w33//e9/p127duzatYvt27fTsGFDxo8fT+3atdm5cycTJky4of0nn3wCwJ49e5g7dy6PPfYYSUlJgH2xrPnz57Nnzx7mz5/PH3/8kaufVylV8ERg4ssXeQL7+kaeL4/K1+MVzURigWu1tkJCQvD392fo0KEABAQE0KpVKwC2bNlCZGQkoaGhBAcHM2PGDI4dO8aBAweoWbMmderUwRjDI488kuEx1qxZw4gR9pXO3N3d8fPzyzKmX375hUGDBgH2gpABAQEcOnQIgE6dOuHn54eXlxcNGjTg2LFjeXIelFL5b9UqaPrrp5TgMikdOkNwcL4ez5lKpBSc7F4XGj7c/sgDma1H4uvre11YQufOnZk7d+4NbXbu3Jkv5eOzuj7m6emZ/r27uzspKSl5fnylVN4TgdfHCpP4FoBiL+X/0LT2SJxIq1at2LhxI4cPHwYgISGBQ4cOUa9ePY4ePcqRI0cAbkk013Tq1IlPP/0UAJvNxqVLl7IsMd+2bVvmzJkDwKFDhzh+/Dh33XVXXv9YSqkCtHEjbN5iuN9vHYlzF0OnTvl+TE0kTqRChQpMnz6dAQMGEBQURKtWrThw4ABeXl5MmTKF7t2706ZNGwIyuWj20UcfsXbtWgIDA2nWrBn79u2jXLlyhIaG0qhRI0aNunGc9Omnn8ZmsxEYGMhDDz3E9OnTb+iJKKUKn/fes399ath5vPs/kOvruTmh03+VUspFREbCfQ2PkeLpw44jPtxR1ff2b0qj03+VUkrx8cfwLi8SlVydO7b8UGDH1USilFIuIDYWVs2Kpg+LKEYytMiwNGG+0ESilFIuYNYs6J/wBcVJxtx/P1SvXmDHLprTf5VSyoWIwGef2PiO/9k3pN1PVlC0R6KUUoXczz9DzQMrCOA4UqsWdO5coMfXRKKUUoXc5MkwAvs9ZOappyAbxV/zkiaSAlKiRIkbnl9fbPGzzz5j5syZWb7/+va3c+0+lEmTJvHqq6+yevXqW9pcX4o+M5mVq1+yZAmRkZHZiiUj1xeOzKrNV199letjKFVUnDwJyxddpTjJiKcnPP54gceg10gyMGcOvPyyfT1jf3/7KmJ5Wbv/Zk899VSe7ev06dNs2rQpz2tj9ezZk549ewL2RNKjRw8aNGiQp8e43rVE8vDDD+fbMZRyBVOnQqKtOJ91m8E9Mz2hXLkCj0F7JDeZM8deXuvYMfsFrGPH7M/TKonki7FjxzJx4kQAwsPDCQoKonXr1owaNeqGv9xPnjx529Lu9957L2fOnCE4OJgNGzYwePBgvvnmGyDzUvSXL19myJAhNG/enCZNmvDtt9/est9rPaJNmzaxdOlSRo0aRXBwMEeOHKFp06bp7X777TeaNWt2y/u3bdtG48aNad26dXrVYbAnjLCwMJo2bUrTpk3ZtGkTAKNHj2bDhg0EBwczadKkTNspVZSlptoTCcDTQ85ZkkSAvxZUcpVHs2bNxBEBASL2FHLjIyDAod2Km5ubNG7cOP1RvXp1GTlypIiIvPbaazJhwgQREWnYsKFs3LhRRERefPFFadiwoYiITJs2TWrWrCkxMTGSmJgo/v7+cvz48VuOc/To0fT3iIg89thj8vXXX0tiYqJUq1ZNDh06JKmpqdKvXz/p3r27iIiMGTNGZs2aJSIiFy9elDp16kh8fLysXbs2vc20adPS4722z2vat28vO3bsSN/Xxx9/fEtcgYGBsm7dOhEReeGFF9JjvHz5siQmJoqIyKFDh+Tav9/1x86qnVJF2erVIvWIlD4V14ot4YJD+wIiJJefu9ojucnx4znbnl3Xqv9ee4wbN+6WNjExMcTFxXH33XcD3DKs40hp96xK0a9atYrx48cTHBxM+/btSUpK4ngOfuBhw4Yxbdo0bDYb8+fPvyXu2NhYYmJiaNeuHUB66XqA5ORknnjiCQIDA+nXr1+m116y206pomTGDPg/3mNhdAfc/jvVsjj0GslN/P3tw1kZbc9vcpu6ZxmVdl+8eDGvv/46AFOnTqV8+fKZvj+zUvQiwsKFC2+p/BsdHZ2tuB988EFef/11OnbsSLNmzSh3U/daRDI99qRJk6hYsSK7du0iNTUVLy8vh9opVVTExcEP38Qzma/tG3r1siwW7ZHc5K23wMfnxm0+Pvbt+a1MmTKULFmSLVu2ADBv3rzbvqd3797pvZyQkMzrrWVVir5Lly785z//SU9kO3bsyPKYN5em9/LyokuXLowYMYLHM5gxUrp0afz8/Pjll18A0kvXg723UrlyZdzc3Jg1axY2my3DY2TWTqmiauFCuC9xISW4DHe3hrp1LYtFE8lNBg60r7AbEGCvvhwQYH+en7O2rvfFF18wfPhwWrdujYjcdpXD7MqqFP0rr7xCcnIyQUFBNGrUiFdeeSXLffXv358JEybQpEmT9MQ0cOBAjDHce++9Gb5n2rRpjBw5ktatW+Pt7Z2+/emnn2bGjBm0atWKQ4cOpS/0FRQURLFixWjcuDGTJk3KtJ1SRdWMGTCY6fYnjw+xNBYtI+9k4uPj0+85GT9+PKdOneKjjz6yOKrbmzhxIrGxsbzxxhtWh6KUy4uKgo41f+d3aiNeXpjTp8HBPzodKSOv10iczHfffcc777xDSkoKAQEBTJ8+3eqQbqt3794cOXKENWvWWB2KUkXCrFkwiFkAmAcfdDiJOEoTiZN56KGHeOihh6wOI0cWL15sdQhKFRkiMHMmPIgnV0qVwfPRR60OSa+RKKVUYbJlCxw+DDMrPkexY3sKZE3229FEopRShci1yZz9exzDvXRVcHe3NiA0kSilVKFhs8HCBTYeYzoDu5y2Opx0mkiUUqqQ2LAB6p7+mek8TtNXh1sdTjpNJAXk5jLyeWnDhg00bNiQ4OBg/vzzT/r27Zthu/bt23O7qdHXt+nWrRsxMTHExMQwefJkh2K8vnhkZqZPn87JkycdOo5Srmz+fHgY+/IKpm8/i6P5i87ausnGShtJjk6+ZbtHRQ9CT4daENHtzZkzhxdeeCH9rvLbfWBn14oVKwB7hd7Jkyfz9NNP58l+MzN9+nQaNWpElSpV8vU4ShVGycmw7Osk3iHt/7cTLbHgND0SY8yXxpgzxpi9120ra4z50RjzW9rXMvkdR0ZJJKvtjjh27BidOnUiKCiITp06cfz4cWw2G7Vq1UJEiImJwc3NjfXr1wMQFhbG4cOHb9jH1KlTWbBgAePGjWPgwIE3LBqVmJhI//79CQoK4qGHHiIxMTH9fatWraJ169Y0bdqUfv36ER8ff0t8NWrU4Ny5c4wePZojR44QHBzMqFGjGDRo0A2l5gcOHMjSpUtveK+I8Mwzz9CgQQO6d+/OmTNn0l8bN24czZs3p1GjRgwfPhwR4ZtvviEiIoKBAwcSHBxMYmJihu2UKqrWrIHm57+nNLFIcGOoX9/qkP6S27LBef0A2gJNgb3XbXsPGJ32/Wjg3dvtx9Hy4mtZm+nDEb6+vrds69Gjh0yfPl1ERL744gvp1auXiIh06dJF9u7dK8uWLZOQkBB58803JSkpSWrUqJHhvq8v6359Gfn3339fHn/8cRER2bVrl7i7u0t4eLicPXtWwsLCJD4+XkRExo8fL6+//rqIiLRr107Cw8NFRCQgIEDOnj17S2n6devWpccaExMjNWrUkOTk5BtiWrhwodxzzz2SkpIif/75p/j5+aXHeP78+fR2jzzyiCxduvSWY2fVTqmi6PHHRRbQ176uRdqyE3kJVygjLyLrgQs3be4FzEj7fgbwQIEGlc82b96cXnJ90KBB6UUNw8LCWL9+PevXr2fMmDH88ssvhIeH07x58xztf/369enl4oOCgggKCgJgy5YtREZGEhoaSnBwMDNmzMhRSfp27dpx+PBhzpw5w9y5c3nwwQcpVuzGUdL169czYMAA3N3dqVKlCh07dkx/be3atbRs2ZLAwEDWrFnDvn37MjxOdtsp5equXIGVC+PpwXL7Bie7adnZr5FUFJFTACJyyhhzR0aNjDHDgeEA/gVR7z2fXCu1HhYWxmeffcbJkycZN24cEyZMYN26dbRt2xawV+uNjo4mJCSEqVOzXoMgo/LtIkLnzp1vqACcU4MGDWLOnDnMmzePL7/8MtvHTkpK4umnnyYiIoLq1aszduxYkpKSct1OqaJg5UrwvnSaSJ8mNAsGqle3OqQbOE2PxBEiMkVEQkQkpEKFClaHk2133313eqn4OXPm0KZNGwBatmzJpk2bcHNzw8vLi+DgYP73v/8RFhYGwMqVK9m5c+dtk0jbtm3TS7bv3buX3bt3A9CqVSs2btyYfr0lISGBQ4cOZbqfm0u6g30W1ocffghAw4YNMzz2vHnzsNlsnDp1irVr1wKkJ4Py5csTHx9/w8SA64+TVTulipqvv4Yj3MmqZz6HteusDucWzp5Ioo0xlQHSvp65TXuHeVT0yNH27EpISKBatWrpjw8++ICPP/6YadOmERQUxKxZs9Kr/Hp6elK9enVatWoF2HsocXFxBAYG5uiYI0aMID4+nqCgIN577z1atGgBQIUKFZg+fToDBgwgKCiIVq1aceDAgUz3U65cOUJDQ2nUqBGjRo0CoGLFitSvXz/D9UfAXsixTp06BAYGMmLEiPTVEUuXLp2+0uEDDzxww3Dd4MGDeeqppwgODsbT0zPTdkoVJcnJsDxtROvBvu5QvLi1AWXAqcrIG2NqAMtFpFHa8wnAeREZb4wZDZQVkf/Lah+FvYx8YZGQkEBgYCDbt2/PszVTlFK3Wr0a/tl5Lw38z7HgSCsolj+rgzpSRt5peiTGmLnAZuAuY8wJY8xQYDzQ2RjzG9A57bmy2OrVq6lXrx7PPvusJhGl8tnixfAi77LgeAf45H9Wh5Mhp+qR5AXtkSilXEVqKtSudoUdpypSmlg4eDDfltR1iR6JUkqpG4WHQ/1TP9lvQgwMtHRd9qxoIlFKKSe1ZAk8yEIATCY19JyBs99HopRSRdayRcn8zBL7EydOJNojUUopJ7R/P1Q+tI5yXEDq1YMGDawOKVNFs0eye2ze7i8o6/2dP3+eTmnLYZ4+fRp3d3eu3Tj566+/Ujwf5oUvWrSIBg0aUK9evVteO3z4MH379mXnzp03bF+9ejX//e9/WbJkSZ7HU61aNfbu3Uvp0qXzfN9KuaLFi+EOznDJsxylnLg3AkU1kRSwcuXKpX9ojx07lhIlSvDCCy/k6zEXLVqEm5tbholEKeX8liyBcAby0Oct6dm7ktXhZEmHtiz09ttvpy8Y9eyzz3LvvfcC9hIogwcPBmD27NkEBgbSqFEjXnrppQz3M2rUKBo0aEBQUBAvvvgiGzZsYMWKFTz33HMEBwcTFRVFeHg4QUFBtG7dms8++yzTmGJjY3nggQdo0KABI0eOTC/dPnz4cEJCQmjYsCHjxo1Lb1+tWjXGjh1LkyZNCAoKSi+1cvbsWTp37kzTpk0ZMWKEloBXKgdOnLDP2PLxTqFz7+qQjwvj5QVNJBZq27YtGzZsAGD79u3ExMSQkpLCL7/8QlhYGCdOnODf//43a9euZceOHWzcuJHl12olpImOjmbFihXs27eP3bt3M2bMGMLCwujWrRuTJk1i586d1KhRg8GDB/Ppp5+yefNmbDZbpjFt3bqVDz/8kD179rB///70dUfGjx9PREQEu3bt4scffyQyMjL9PRUrVmTHjh0MGzaMDz74AIDXXnuNDh06sH37drp27aorHyqVA0uWQAjh9An9De8SnlaHc1uaSCzUvHlzwsPDiYmJoUSJEjRv3pwdO3awYcMGwsLC2Lp1Kx07dqR8+fJ4eHjw8MMPpy9ydU3ZsmVxc3PjiSeeYPHixfj6+t5ynHPnzpGYmEhoqH2Fx0GDBmUaU6tWrahRowbu7u70798/vbT93Llzadq0KU2bNmX//v03JJI+ffoA0KxZM6KiooAbS9j36tWLkiVL5v5EKVXELFmUyhIeYPraxpBFQVVnoYnEQp6enlSpUoWZM2cSGhpKWFgYP/30E8ePH6du3brZGg7y8PAgIiKCBx54gIULF9K9e/cM22VU0j077Ywx/Pbbb3z00UesWbOG3bt307Vr1xtKunt62v9icnd3JyUlJcfHVEr95fx5uPLzFqpyEipXgjp1rA7ptjSRWKxt27ZMnDiRtm3bEhYWxieffEKzZs0Ae+9g7dq1nD9/npSUFObNm5deRfeauLg4Ll26RI8ePZg0aRI7duwAbizJXr58eby8vNi8eTNAemn5jGzZsiV9yd8FCxbQpk0bLl26RMmSJSlVqhSnTp1i5cqV2fq5rh1n2bJlt5ShV0pl7LvvoHeqfdkE9359oRD8QVY0Z23dZrpuQQoLC2PChAm0bNkSb29vPDw80tcdqVatGuPGjaN9+/aICPfff/8tPY7Y2Fj69OnDlStXSE1NTb9GMWDAAJ588knef/99lixZwrRp0xg2bBi+vr7pF/Uzcvfdd/P888+zb98+2rdvT8+ePQFo0KABjRo1olatWulDZFl5/fXXGTBgAAsWLKBDhw5UrVo1t6dIqSJl8SLhw7S72Z35JsTradFGpZRyEgkJ0KVsOBuutMBWsTLuJ0+AW8EMHGnRRqWUcgErV0L3K/beiHu/BwssiTiqcESplFJFwJIlUJ0/7E8KybAWaCJRSimnkJwMy5bBI8zhyOpD0KaN1SFlW9G82K6UUk5m/Xq4eBHq3RlH7U7OP+X3etojUUopJ7B4MTRkL727xVodSo5pj0QppSyWmgr7vo5kL4HE/9gMJLxQ3D9yjfZIlFLKYtu2QZsz9tlavi2DClUSAU0kSillucWLoS/2u9lNv8IzW+saTSRKKWWx7fN/ozG7SfEpBWmL4BUmmkiUUspCBw5A8O/2YS23B3qCp/OXjb+ZJhKllLLQkiXwYFptLbe/Fb5hLdBEopRSlvrx6xhq8Tspnr6QRUFVZ6aJRCmlLPLnn7Bme2lqev5J8s+bwNvb6pByRROJUkpZZMkS+9dOHePwbhlkbTAO0BsSlVLKIj98HUcJoHdvd6tDcYj2SJRSygIXL0Kt9dM5SwX6/jHF6nAcUih6JMaYKCAOsAEpuV18RSmlnMXy5dBHvsGLK1DPP1vv2VhpI8nRybds96joQejp269cml8KRSJJ00FEzlkdhFJK5YU1c6P5gg3Y3Ivj3qNHtt6TURLJantB0aEtpZQqYAkJUHL1YtwQrra/F0qVsjokhxSWRCLAKmPMNmPM8JtfNMYMN8ZEGGMizp49a0F4SimVfT/+CPcn229C9H7kQYujcVxhSSShItIUuA8YaYxpe/2LIjJFREJEJKRChQrWRKiUUtm0au55OrAWm1sx6NnT6nAcVigSiYicTPt6BlgMtLA2IqWUyp2UFIj7bj3FsJHYuiOULWt1SA5z+kRijPE1xpS89j1wL7DX2qiUUip31q+HWfG96egfSYlPJ+bovR4VPXK0vaDkeNZW2od5kojY8iGejFQEFhv7Qi/FgK9E5IcCOrZSSuWpxYvtX1s+WBECc9YbsXKKb1Zum0iMMW5Af2Ag0By4AngaY84CK4ApIvJbfgUoIr8DjfNr/0opVVBEYNWieMCXB/r5WR1OnsnO0NZaoDYwBqgkItVF5A4gDNgCjDfGPJKPMSqllEuIiIB3Tz7CYfe7aE6E1eHkmewMbd0jIrfc7SIiF4CFwEJjjLUDdEopVQgs+yqOMazE03YFtxrZu5u9MLhtj+RaEjHGfGhMxivSZ5RolFJK/UUE4uYux5skLgWGQuXKVoeUZ3IyayseWJp2sR1jzL3GmI35E5ZSSrmWvXuhbfQCAEoO+ZvF0eStbM/aEpF/G2MeBtYZY64Al4HR+RaZUkq5kOVfXeI5vicVg/vfCv/d7NfLdiIxxnQCnsCeQCoDQ0XkYH4Flmu2JKsjUEqpW8TOXoYXV7jQKIyyVapYHU6eysnQ1svAKyLSHugLzDfGdMyXqByREg9JWiRYKeU8fvsNGp9YDkCpYQ9ZHE3ey3YiEZGOIvJL2vd7sNe9ejO/AsstEYGTK6wOQyml0i1aBI8xgwlhCyj2sGtdH4FsJJIsZmqdAjpl1cYK5w9eIWnxari42+pQlFIKsCeSZIpz5z/6gAsWls1Oj2SNMeZZY8wNk56NMcWB1saYGcBj+RJdLpRPiSblxSWk/DIHUhKtDkcpVcT98Qfs+PUqPt42utxXuNdmz0x2Eslv2Je4XWyMOWmMiTTG/J62fQAwSUSm52OMORJr/ChhiyN64DfIoSVWh6OUKuK++yqWaCryY5l++HgWVInCgpWdRHK3iEwGDOCPfTirqYgEiMgTIrIzXyPMoWIBZdlv6lE17nciu38Gl/KtDJhSSt3W+WlLKUMMtf0ugHvR7ZGsNMZsxl6F91GgCuC0c2x9S8Kpl4YRgx8No9bza+93dEqwUsoS0dEQdNB+E6Irzta6JjslUp7HXvnXBtQEXgH2GGP2GWPm53N8OVe8DB2HVmLrgL+TiiFkzXQ2Pf+p1VEppYqg7+fG0IWV2HDDe2Afq8PJN9m6IVFEfjfG3CMih65tM8aUABrlW2S5ZqDGALq8u4Q1Rwbj8etvDPq0L3PuO0pol5pWB6eUKkLOffEtxUnm5F0dqFKxotXh5Juc3Edy6Kbn8SKyJe9DygPGDar1psM3ffmq90SOXa3O/X+rxP7d8VZHppQqIi5cgPr7vgag1DDXu3fkek6/1G6uGYOp3o3//CeJ+9sdJPZScRaEfcLxA5etjkwpVQQsnxvHPfIjNuoiu1sAABsoSURBVNwo8ajrDmuBKyeSNMWqtmPerMssKD+E1y6NJrLZI5z6M9XqsJRSLm72tyWpxwE2DJkGd9xhdTj5yuUTCYBP9aZ0/upB4txK0TVhCT8F/p1zZ8XqsJRSLursWVizRjhRLICgCY9aHU6+KxKJBKBU556kzppMMh48cvETvgkaS2ys1VEppVzRogUp2Gxwb6erlC1rdTT5r8gkEgC/hwcS/9/J2HDjqdPjmBb8IZf1kolSKo+lfvAhkTRgVN2iUUC2SCUSgDIjh3Hx7Y8A+GfUc7zbapEmE6VUnjl5Elr+/hX1OUDzFk5TzzZfFblEAlB+zDOcfXE8mz1C+XDvPdx3H8TFWR2VUsoV/PTf/TRlB5eL+eHbr5vV4RSIIplIACqMf5GyEasoVcmdDRvg3s5CTIzVUamiKDUV4uMhJgYSEsDmmnX9igzbrDkAnG7TFzw9LY6mYGR7qV1XdFeQD+s32OjcPpa/b32Kz4LaMHTbSFdcLkBZJDkZjhyBgwfh5OZjuP+6GU6fxuP8abzjovG8Ekfx1ES8SOIeVmOvjQqvm7FU9blIQtmq2CpWxb1+XSreE0jjll7UqQNuRfZPQOf2x3Gh7YmvAKgyaqDF0RScIp1IAGrd6c7mtzdzx2Pz4I95fFjvAj22/Js76xSNsU2Vd+LjYffPFzmxbAdXtuygxNE9TI3vz4rUrgA8wUqm8GSm7y9fMon4ZC+uXIGusoIWl8PhMvAHEAEps9zZT33e8HyYvfePoVs36NoVKlcumJ9P3d7697YwkKNc8KpC2S5trQ6nwBT5RAJwx6NdiT39GSVfHME/L7zK7MBjXPjpU1qEelgdmnJSV6/C7t0QHg7lv3iXMoe2UjtuB3cTdUO7fVQjMqAz9eoZ7irdhCMH+2CqVqF49Ur41KqITyU/ipf2wc3Xm7PtioO7/Q+Y5KWvc+HX/SQd+RNb1B94HtlHubMHCGQvi64k8c038M03cCe/8Y/a31Hp+YF0H1wBb28LToZKZ5tnL4ly4d7+lHXRkvEZMSKudWNeSEiIRERE5Oq9ifO+xQwcgFdqIj+6deHK7K/pMaBkHkeoChubDQ5vPc+xhREk/RKBx+H9PBA3k6vJ9vGlnTSmMfalnRONN6cqBHGlfhNKtAmmwgOheIXkUW3ThATYs4fjtqos31mNFd/ZCFr1Pm+nvMhVPPjeoycnezxJr//cQ5Wq2qMuaLt2QfPgqzzku4wvfm1M8QZ3Wh1SjhhjtolISK7eq4nkRikbt5JwTw9KJZ1jHw1Y/o/VvPB+ZVddj0bdJDkZ9u+HAz/+QfHF8yh5IIJaF8KpKUdvaBdAFN53VadFCzcedptH7Zo2qt/fBK+gulCs4Dr68cvXcf6liVTb8z3u2Ev/RJgQtt/3b7p/dj9Vq+vFlILywgvw/vsw4qlUJn9a+M67yycSY0xX4CPAHZgqIuMza+toIgGQw0c4H9qDPWcq0Zkf6dy1GHPmUCTuUC0qbDY4vieWkz/tJ37LXtwi97I1IZA3Tw3hyhVDazaxidD09onGm6iyTblcLwTvsBCqjbgfP38/C3+Cm5w8ycm3puHzxceUvnIGgLluA4l8aTZjxoCPj8XxuTibDepUS+ToaW82bYLWra2OKOccSSSIiFM/sCePI0AtoDiwC2iQWftmzZpJnoiLk58XnJJyZZIEREKqn5INPybmzb5d0OzZIgEBIsbYv86ebV0sKSki0VEJcnDlUdn2yWaZOz1J3n5b5MknRWbXflV2e4XIOcqKwA2PZXQXEKld2yYDe1+WbS1HyJ5/fSnn1+0WSU627gfKicuX5c/RH8lZ72oSygYB+7/HksWpkppqdXCua93sPyQOX/mm5KOSaiucJxqIkFx+Tjt9j8QY0xoYKyJd0p6PARCRdzJqnxc9kusdOwb9esUxflcvKnOKnx6ZzhNTWxaV6eHZMnNGKk89BYlJf3XnvYql8GL7LXSseohj1UI5X/4ubDYof2wbNfevQFJs9j/jUlMh9dpXYXm7CYjYn3ba8hal445jUlMxaW2M2L/uKteJFZWGcPkyVD6zixcOP4VPyiV8bbGUTI2lJH+tPVOXg/xGXXusDGIQswFIxIs/S9zFxSqNSKnfCJ/2LanxeAf8nKijkWspKWzcWoyRI4VduwxTGYpX/Zp0WTOa8pV0jk1eW9DkHf628yX2NehHw30LrA4nV1x6aMsY0xfoKiLD0p4PAlqKyDPXtRkODAfw9/dvduzYsTyN4erx08Q0accdFw5hw40Z5Z+n0ddjadHe9ccLrsZf5dTW45wLP8rlvUdJPfw7HqePc9JWibGlPiA6Gi6cT0UyuLc1gCiiqMkwPucLhgHwFJ/yKU9neKxUTPo4P8B2mtCEnRm2/YwnGcFnADRlG9u48ff/Kh6cL1aRGK9KfH73dNyDGhIQAIGpu6jid5lKrWtSsk4lMK59UTolBeaNO8Qjb9wFwHaPFlyePJOwYXdZHJnriI8TTvrVo64c4tTU76g8tHDeze7qiaQf0OWmRNJCRJ7NqH1e90jSJSZyctgrVPxqEu6k8gfVWNHmHXrOe5jKVQvXhbU5c+Dll+H4cfD3h7fegn69rnL0u0h2xdVi19FS7NsH9//8PINjPrzhw/2a/dSjAfvTngnXbqS7niGV9XWGEtFwMEf92+HuDjXO/Eqj378F92Lg7m5/uLlBMXeMmxvhbZ/Hzd3g5gZ1d32NT+J5cHdH3NzAzR3jbm971b8OSUEt8PWFkm6XKXdiF8XLl8K3cilK+5eiWDk/l08SOXF69mrchj7OHVdPkIA3KzpMoOf3T1PcU8+Ro354dRNd3wjlnEclyif8UaCTLfKSq18jaQ2svO75GGBMZu3z7BpJJhLWbJY/72icPq7+s3t7eX1sqly4kK+HzTOzZ4v4+KTecHnAiwSZziARkPv4Ln37aN4WG0b+cPeXHX7tZP2dg2Vtx3Gy4YkZEj5xnezaJXL6tIi//y2XG9LH5pXzSDl3UfYEP5L+D/Rjhf4S/Xu81WEVesurPiECsqPzKKtDcQgOXCOxPFHcNkD7TZO/AzX562J7w8za53ciERERm02i350m5z0rycu8ISBSsqRNXno+SU6fcK6Lsqm2VDm6NVrmzhV5/nkRL09bxh/6HJXfPerK+FaL5d//Fpk7V2TvljhJik267THsyenG/fn4WHvBXWXu0JvzJc6UEAH5tMQLEhFhdUSF19Hdl+QS9nN5cVOk1eE4xJFE4vRDWwDGmG7Ah9hncH0pIm9l1jbfhrYycvky6zfAG+8Kq9eV4Dk+4DkmEVH3Yco88wihTwXiUYA3x0uqcGrrcU4siSDxl22UPBBOrYsRXJTS1OJolu81RkhNzf0wR0bDZQOLTqmhQufs+v3s7z2G7hdmkeJVgpkzDf36WR1V4TPnoaUMXNCL/Xe05YJ5m+To5FvaeFT0IPR0aAbvdi4ufY0kpwo0kVxn61bw6tGBxufWpW+LdA/kaP1ueHfvSKMnQ7mjpm+eHS8lMZmoQ1fZF+XL9u3gu3QuQ3b9nfJy7pa20W6VGNn5NwJbl+CTT+zLgN4sIACiovIsPFUIXL0KzzwDn38OniQxssM+Fv7eTP8YyKaUFAgIELxPHmH+lEvEDb+Uadv20r7gAsslTSTXsSqRAJCayrklv3DivTnUDP8av9SL6S9NZSgT75pCYKAbd9c8RQvzKyVqVaR03TsoW7c8Xn6eFPO0X0xOSXUjLs6+RkrC5l3E7znKlSMnsB0/gVfUQSqc20+1q0cYy1je5mUAOrOKVXThvCnH72VDiLsrBO/QZvg/2Jwqzati3Oy9jTlzYPhwe7WNa3x8YMoU/dAoikTg3fHCsZcmM5PBJPDXHzv6e5G1ZcugZ0+oW8fGgYPu/Oy2LtO2rp5ICuf0Amfl5kb5Pm0p36ctcuU//P7lWs4tWIPf9p/YkNiBgwfdOHgQvFjNczya4S6SKUZxrnJtFtQuBhHCnlvapWKoVeIM994NjRtDy6AwjgdEUT3Un3JumQ9RXftQ0GEoBfaJbaPHGCq99SgJl2/sMSck2H9P9HcjY8s+tN8nPewJ9yI/QVB7JAUkKclew2nPHkhZtJSgzZ/hc/ksfklnKG07hwfJuGPDnVR8uExxP29KlYIP44ZRxeMMieWqkVqpKh4N6lAutB4BnetSooKWelV5w83N3ju5mTH2m0PVjU7uvUDZwCocpB6Vjm6mYg1v1pl1mbbXHonKE15e0KSJ/cGjPYGet7QRAVsqXDaSPhQFXxRkmKqI8ve3V3G4WfVqqRThhVQzte//ptOZK6RWrETFGvoHnf6GOBFj7PfnmSyGppTKD2+9dWthRx8u0yt5Icm3TkQq0lJtwp0/2qsqyFMj0rd7VMx4imZm212J9kiUUrdcO6tU7ir9LnzJ56eHcu4xmDULXUohTcRbK2mR8hun3KsS/FL39O2FYYpvftEeiVIKsCeTqCj7NZGTZ4vzyJZncS/hzdy58MyTyciVq1aH6BSKffQ+AIc6P4Nbcf1bHDSRKKUy0bw5LF9uKOWZxL1f/I29jfojV4v2ONeRRbtoemE18fgS9MmTVofjNDSRKKUy1a4dLP34GO1ZR+DhxUQ2G2Qv/19ErfrfUc5RjvDAoZSpVcbqcJyGJhKlVJbaDb+LbW+v4hIlabh3PpGthxTJOcHnz8PzG3rhz3GqTnnN6nCciiYSpdRt3TOmOetHf088vjQIn8mB+57L+MYTF/b555CYaGjX1Ye6rXTd7etpIlFKZUuPd0L5YcRSrlCceqs+5rehGS5S6pKSL8aTMP4jShDHP/9pdTTORxOJUirb+k7uyPz755CK4c+ZP7EjPMXqkArE3n9OZVzsP/nO92/ce6/V0TgfTSRKqRx5ZElf3m33PV1sK+jW093lq0anxidQ7at3AUh4dESRr6uVEU0kSqkccXODf63sQmgHT06fNvTocoWYX/ZaHVa+iXz2UyqknGaXRzM6fHC/1eE4JU0kSqkc8/SExYshpGEi7x3qTbH2oSRt2Wl1WHlO4uKpMtveGzk+dByeXtodyYgmEqVUrvj5wZIVxUn1LkEJ2yUS23fFduiI1WHlqf3PfkLZlLNs82hJ5w/uszocp6WJRCmVa1X93am9aRbrinWizJVozjfvgpw6bXVYeUIuxVFl9nsAHBsyDi9v7Y1kRhOJUsoh9YM9Kb58MdtMM+64dIToZvdBbKzVYTnsh19K8JjtS2Z5D6Pr+52tDsepaSJRSjns7i4lOTV1BYeoQ6VTOzndshckJlodVq6JwNixNpbSizNvfI6Pr/ZGsqKJRCmVJ3oMuYPNr63kT6qw46AP368ovHe+L5saza/hxahYUXjqKaujcX6aSJRSeeaxsTX5asQv9GIJDw7yZvNmqyPKueSd++g63J//8AyvjwVf39u+pcjTRKKUylMvfFKTQUOKk5ho6NMtgT/fmlZ46nKJcKrfsxTnKuVKpzJ0mA5pZYeuyqKUylPGwP/+B2fPCE8u70fVf68g9sIf+L3/qtWh3VbM5K/wP7yWc5Sj7CdvUEw/IbNFeyRKqTxXrBjMm2/YWG8oNtzw++A1Lr/zkdVhZe3iRcwL/wJgdtAEujxczuKACg/Nt0opADZW2khy9K0rIHpU9MjVeuQ+PvDCxj68HjiFcSeH4fvSP0ks7oH380/nRbh57nT/f1Ip6Qy/mDDu/+Yxq8MpVDSRKJXP8voDOr9kFGNW27OjbFkYvnUorzZOYtyFZ/B+YSRXPNzx/LtzLVOb9PUyKq2aSQLe7HxmKm3q6GBNTujZUiqf5ccHdGFSrRo8HjGS1/w+BMDzH09xZcn3Fkd1o7dWNeFr+vKfKu8wfGJdq8MpdLRHopTKdzVrwsBf/8FrzWw0iV/Pl5PbsKAreHlZHRn8uCqVN6dWw919AVu/heLFrY6o8NEeiVKqQNStC3/b8i+Gl1vEsh9L0v2+q8SfS7J0anDstEU88Ug8AK+9ZmgWotN9c8OpE4kxZqwx5k9jzM60RzerY1JK5V7DhrBmnRuVKsHGdakcqH0fScOfhdTUAo/FNmcefkMeZNbZ7rQPs/HSSwUegstw6kSSZpKIBKc9VlgdjFKuyqOiR46251ajRrBhA3SrtINGlzbjNfUTkrr1hkuX8vQ4Wdq4EdtjjwOwskRfZn3ljrt7wR3e1eg1EqXymUdFj0xnbTmTgpxBdued8NGvrXni7h/46EQfyq5cSmJwK7xXfgt16uTvwbdt48o93fC0JTHF7Unu+/7vVKuWv4d0dUacuHSBMWYsMBi4BEQAz4vIxQzaDQeGA/j7+zc7duxYAUaplMqtM2cgoup6fFJuHdrKl+nRmzdzpXM3PC/HsIB+JH4xl8eGaFcEwBizTURCcvVeqxOJMWY1UCmDl14GtgDnAAHeACqLyJCs9hcSEiIRERF5HqdybYXlXg9XtM6sy1H7XP+bHDxISmATiiUnsoReHHl7Ac+P0Sla1ziSSCwf2hKRe7LTzhjzObA8n8NRRVRRv9ejMMnNv4kIvDOrFA2TO3OO8vz5yv94dYzlH38uw6nPpDGmsoicSnvaG9hrZTxKqUJm0yZibN48/nYVlvxQGW/m8s4kb179p07zzUtOnUiA94wxwdiHtqIA56qroJRyPikpsGYN8sEHmJUrOVysJd+mbMLPD2bP9qFHD6sDdD1OnUhEZJDVMSilnNA998CwYdC/v/35gQOwaBHs3Enq2nW4nTuLAeLxZWVKJzq0SWHK9OLUrm1p1C7LqROJUsr1ZTY9Oks//cSSPwPZ/mtLSvgkU2v/Gvouehmw3xx3iDrMYhBLq4zg7+PK8+Pj4FYY7porpDSRKEXhudfDFWU2AyuzmXSxuPMg33D4wJ3sPlATgNq48TujOERdNhFKxXb1eGywIfxhrZ1VECyf/pvXdPqvUq4tORm2b4fISDh6FBIT7b2NypXt9bxat4YyZayOsvAp1NN/lVIqJzw8oGVL+0M5Bx01VEop5RBNJEoppRyiiUQppZRD9BqJylNas0qpokd7JCpPac0qpYoe7ZGoHNEeh1LqZtojUTmiPQ6l1M00kSillHKIJhKllFIO0USi8lRmtam0ZpVSrksvtqs8pRfclSp6tEeickR7HEqpm2mPROWI9jiUUjfTHolSSimHaCJRSinlEE0kSimlHKKJRCmllEM0kSillHKIJhKllFIO0USilFLKIXofSQ5lVkYdNyD11s1aXl0p5eq0R5JDmZZLzyCJZNleKaVchCYSpZRSDtFEopRSyiGaSJRSSjnE8kRijOlnjNlnjEk1xoTc9NoYY8xhY8xBY0wXq2JUSimVOcsTCbAX6AOsv36jMaYB0B9oCHQFJhtj3As+vBtlWi49kzOp5dWVUq7O8um/IrIfwBhz80u9gHkicgU4aow5DLQANhdshDfSqbxKKXUjZ+iRZKYq8Md1z0+kbbuFMWa4MSbCGBNx9uzZAglOKaWUXYH0SIwxq4FKGbz0soh8m9nbMtgmGTUUkSnAFICQkJAM2yillMofBZJIROSeXLztBFD9uufVgJN5E5FSSqm84sxDW0uB/sYYT2NMTaAO8KvFMSmllLqJ5YnEGNPbGHMCaA18Z4xZCSAi+4AFQCTwAzBSRGzWRaqUUiojRsS1LikYY+KAg1bH4STKA+esDsJJ6Ln4i56Lv+i5+MtdIlIyN2+0fPpvPjgoIiG3b+b6jDERei7s9Fz8Rc/FX/Rc/MUYE5Hb91o+tKWUUqpw00SilFLKIa6YSKZYHYAT0XPxFz0Xf9Fz8Rc9F3/J9blwuYvtSimlCpYr9kiUUkoVIE0kSimlHFJoE4kxpmvaOiWHjTGjM3jd0xgzP+31rcaYGgUfZcHIxrn4lzEm0hiz2xjzkzEmwIo4C8LtzsV17foaY+TmNXBcSXbOhTHmb2m/G/uMMV8VdIwFJRv/R/yNMWuNMTvS/p90syLO/GaM+dIYc8YYszeT140x5uO087TbGNM0WzsWkUL3ANyBI0AtoDiwC2hwU5ungc/Svu8PzLc6bgvPRQfAJ+37EUX5XKS1K4l9/ZstQIjVcVv4e1EH2AGUSXt+h9VxW3gupgAj0r5vAERZHXc+nYu2QFNgbyavdwO+x140txWwNTv7Law9khbAYRH5XUSuAvOwr19yvV7AjLTvvwE6mQwWPXEBtz0XIrJWRBLSnm7BXgDTFWXn9wLgDeA9IKkggytg2TkXTwCfiMhFABE5U8AxFpTsnAsBSqV974eLFogVkfXAhSya9AJmit0WoLQxpvLt9ltYE0l21ipJbyMiKUAsUK5AoitY2V63Jc1Q7H9xuKLbngtjTBOguogsL8jALJCd34u6QF1jzEZjzBZjTNcCi65gZedcjAUeSav7twJ4tmBCczo5/TwBCm+JlOysVZLt9UwKuWz/nMaYR4AQoF2+RmSdLM+FMcYNmAQMLqiALJSd34ti2Ie32mPvpW4wxjQSkZh8jq2gZedcDACmi8j7xpjWwKy0c5Ga/+E5lVx9bhbWHkl21ipJb2OMKYa9u5pVl66wyta6LcaYe4CXgZ5iX77YFd3uXJQEGgHrjDFR2MeAl7roBffs/h/5VkSSReQo9mKndQoovoKUnXMxFHu1cURkM+CFvaBjUZOrdaAKayIJB+oYY2oaY4pjv5i+9KY2S4HH0r7vC6yRtKtJLua25yJtOOd/2JOIq46Dw23OhYjEikh5EakhIjWwXy/qKSK5LlbnxLLzf2QJ9okYGGPKYx/q+r1AoywY2TkXx4FOAMaY+tgTSVFct3sp8Gja7K1WQKyInLrdmwrl0JaIpBhjngFWYp+R8aWI7DPGjAMiRGQp8AX27ulh7D2R/tZFnH+yeS4mACWAr9PmGxwXkZ6WBZ1PsnkuioRsnouVwL3GmEjABowSkfPWRZ0/snkungc+N8Y8h30oZ7Ar/uFpjJmLfSizfNr1oNcADwAR+Qz79aFuwGEgAXg8W/t1wXOllFKqABXWoS2llFJOQhOJUkoph2giUUop5RBNJEoppRyiiUQppZRDNJEopZRyiCYSpZRSDtFEolQ+S1vnonPa928aYz62Oial8lKhvLNdqULmNWCcMeYOoAngclUFVNGmd7YrVQCMMT9jL1PTXkTirI5HqbykQ1tK5TNjTCBQGbiiSUS5Ik0kSuWjtNXl5mBfee6yMaaLxSEplec0kSiVT4wxPsAi4HkR2Y99id+xlgalVD7QayRKKaUcoj0SpZRSDtFEopRSyiGaSJRSSjlEE4lSSimHaCJRSinlEE0kSimlHKKJRCmllEP+H7CKZUZ1B9hTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "\n",
    "# Plot\n",
    "plt.figure(1)\n",
    "plt.plot(X_star, y_star, 'b-', label = \"Exact\", linewidth=2)\n",
    "plt.plot(X_star, y_pred, 'r--', label = \"Prediction\", linewidth=2)\n",
    "lower = y_pred - 2.0*np.sqrt(y_var[:,None])\n",
    "upper = y_pred + 2.0*np.sqrt(y_var[:,None])\n",
    "plt.fill_between(X_star.flatten(), lower.flatten(), upper.flatten(), \n",
    "                 facecolor='orange', alpha=0.5, label=\"Two std band\")\n",
    "plt.plot(X_H,y_H,'bo', label = \"High-fidelity data\")\n",
    "plt.plot(X_L,y_L,'ms', label = \"Low-fidelity data\")\n",
    "plt.legend(frameon=False,loc='upper left')\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([lb[0], ub[0]])\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
